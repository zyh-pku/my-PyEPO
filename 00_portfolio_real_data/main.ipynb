{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100 80GB PCIe MIG 1g.10gb\n",
      "Set parameter LicenseID to value 197246\n",
      "Set parameter GURO_PAR_SPECIAL\n",
      "Set parameter TokenServer to value \"license.rc.princeton.edu\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from config import MARKET_MODEL_DIR,MARKET_MODEL_DIR_TESTING\n",
    "from model_factory import build_market_neutral_model, build_market_neutral_model_testing\n",
    "import pickle\n",
    "with open(MARKET_MODEL_DIR, \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "with open(MARKET_MODEL_DIR_TESTING, \"rb\") as f:\n",
    "    params_testing = pickle.load(f)\n",
    "    \n",
    "import os\n",
    "#个人许可证\n",
    "#os.environ['GRB_LICENSE_FILE'] = os.path.expanduser(\"~/gurobi/gurobi.lic\")\n",
    "\n",
    "\n",
    "# ## 系统Gurobi\n",
    "os.environ['GUROBI_HOME'] = '/usr/licensed/gurobi/12.0.0/linux64'\n",
    "os.environ['GRB_LICENSE_FILE'] = '/usr/licensed/gurobi/license/gurobi.lic'\n",
    "\n",
    "# 清除个人WLS许可证\n",
    "for var in ['WLSACCESSID', 'WLSSECRET']:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n",
    "model = build_market_neutral_model(**params)\n",
    "model_testing= build_market_neutral_model_testing(**params_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 7, 'A': array([[1., 1., 1., 1., 1., 1., 1.]]), 'b': array([0.]), 'l': array([0., 0., 0., 0., 0., 0., 0.]), 'u': array([1000000., 1000000., 1000000., 1000000., 1000000., 1000000.,\n",
      "       1000000.]), 'risk_f': array([0.35320865, 0.42288222, 0.37707359, 0.35261655, 0.41933761,\n",
      "       0.32860565, 0.38218537]), 'risk_abs': 1.5, 'single_abs': 0.1, 'l1_abs': 1.0, 'cov_matrix': array([[4.67496276e-05, 4.15905709e-05, 3.60928100e-05, 3.48342604e-05,\n",
      "        4.16919822e-05, 3.34891585e-05, 3.66772287e-05],\n",
      "       [4.15905709e-05, 5.97804688e-05, 4.37975936e-05, 4.19857337e-05,\n",
      "        5.10524222e-05, 3.90648444e-05, 4.49568886e-05],\n",
      "       [3.60928100e-05, 4.37975936e-05, 5.26604548e-05, 3.92702956e-05,\n",
      "        4.44118365e-05, 3.45134604e-05, 3.79007596e-05],\n",
      "       [3.48342604e-05, 4.19857337e-05, 3.92702956e-05, 4.25771017e-05,\n",
      "        4.21567423e-05, 3.38763419e-05, 3.56807453e-05],\n",
      "       [4.16919822e-05, 5.10524222e-05, 4.44118365e-05, 4.21567423e-05,\n",
      "        5.69786874e-05, 3.93194085e-05, 4.43441724e-05],\n",
      "       [3.34891585e-05, 3.90648444e-05, 3.45134604e-05, 3.38763419e-05,\n",
      "        3.93194085e-05, 3.83241775e-05, 3.39340812e-05],\n",
      "       [3.66772287e-05, 4.49568886e-05, 3.79007596e-05, 3.56807453e-05,\n",
      "        4.43441724e-05, 3.39340812e-05, 5.84057733e-05]]), 'sigma_abs': 2.5, 'turnover': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print( params_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Precomputed mode enabled. Skipping time series processing and solution computation.\n",
      ">>> Precomputed mode enabled. Skipping time series processing and solution computation.\n"
     ]
    }
   ],
   "source": [
    "from io_utils import load_dataset_dict, create_dataset_from_dict\n",
    "from config import DATASET_DICT_PATH, TEST_DATASET_DICT_PATH\n",
    "dataset_dict = load_dataset_dict(DATASET_DICT_PATH)\n",
    "dataset_train = create_dataset_from_dict(dataset_dict, model)\n",
    "test_dataset_dict = load_dataset_dict(TEST_DATASET_DICT_PATH)\n",
    "dataset_test = create_dataset_from_dict(test_dataset_dict, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可选模型的信息\n",
    "#  model_info = {\n",
    "        'TCN': {\n",
    "            # 训练配置\n",
    "            'use_mixed_precision': True,\n",
    "            'gradient_clip_norm': None,\n",
    "            'training_reason': 'TCN架构稳定，无需梯度裁剪。混合精度加速训练。',\n",
    "            \n",
    "            # 推荐条件\n",
    "            'recommend_when': [\n",
    "                ('sequence_length > 200', '长序列，TCN处理效率最高'),\n",
    "                ('50 <= sequence_length <= 200', '中等长度序列的最佳选择'),\n",
    "                ('computational_budget == \"medium\"', '平衡性能和资源消耗'),\n",
    "                ('prefer_speed == True', '训练速度快，并行化好'),\n",
    "            ],\n",
    "            \n",
    "            # 适用场景\n",
    "            'best_for': [\n",
    "                '长时间序列预测',\n",
    "                '实时推理场景',\n",
    "                '需要快速训练的项目',\n",
    "                'GPU资源充足的环境'\n",
    "            ],\n",
    "            \n",
    "            # 优势\n",
    "            'advantages': [\n",
    "                '✅ 梯度稳定，几乎不会爆炸/消失',\n",
    "                '✅ 并行训练，速度快',\n",
    "                '✅ 感受野可控，能捕获长期依赖',\n",
    "                '✅ 架构成熟，调参简单'\n",
    "            ],\n",
    "            \n",
    "            # 劣势\n",
    "            'disadvantages': [\n",
    "                '⚠️ 参数量相对较多',\n",
    "                '⚠️ 对短序列可能过拟合'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'DLinear': {\n",
    "            # 训练配置\n",
    "            'use_mixed_precision': gpu_memory_gb < 8 or prefer_speed,\n",
    "            'gradient_clip_norm': None,\n",
    "            'training_reason': 'DLinear极简稳定。小GPU推荐混合精度节省内存。',\n",
    "            \n",
    "            # 推荐条件\n",
    "            'recommend_when': [\n",
    "                ('has_trend and has_seasonality', '数据有明显的趋势和季节性模式'),\n",
    "                ('sequence_length < 50', '短序列，简单模型可能更有效'),\n",
    "                ('computational_budget == \"low\"', '计算资源有限'),\n",
    "                ('prefer_speed == True', '极快的训练和推理速度'),\n",
    "            ],\n",
    "            \n",
    "            # 适用场景\n",
    "            'best_for': [\n",
    "                '有明显趋势的时间序列',\n",
    "                '计算资源受限的环境',\n",
    "                '需要可解释性的场景',\n",
    "                '作为强基线模型'\n",
    "            ],\n",
    "            \n",
    "            # 优势\n",
    "            'advantages': [\n",
    "                '✅ 极简架构，几乎不会过拟合',\n",
    "                '✅ 训练超快，参数量少',\n",
    "                '✅ 对趋势和季节性敏感',\n",
    "                '✅ 可解释性强',\n",
    "                '✅ 内存占用极少'\n",
    "            ],\n",
    "            \n",
    "            # 劣势\n",
    "            'disadvantages': [\n",
    "                '⚠️ 对复杂非线性模式建模能力有限',\n",
    "                '⚠️ 可能在复杂数据上表现不佳'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'Transformer': {\n",
    "            # 训练配置\n",
    "            'use_mixed_precision': True,\n",
    "            'gradient_clip_norm': 1.0,\n",
    "            'training_reason': 'Transformer计算密集，强烈推荐混合精度。轻微梯度裁剪保险。',\n",
    "            \n",
    "            # 推荐条件\n",
    "            'recommend_when': [\n",
    "                ('computational_budget == \"high\" and sequence_length < 200', '充足资源且序列不太长'),\n",
    "                ('need_attention == True', '需要注意力机制和解释性'),\n",
    "                ('complex_patterns == True', '数据有复杂的长期依赖关系'),\n",
    "            ],\n",
    "            \n",
    "            # 适用场景\n",
    "            'best_for': [\n",
    "                '需要注意力可视化的场景',\n",
    "                '复杂的多变量时间序列',\n",
    "                '研究和实验项目',\n",
    "                '对精度要求极高的场景'\n",
    "            ],\n",
    "            \n",
    "            # 优势\n",
    "            'advantages': [\n",
    "                '✅ 强大的序列建模能力',\n",
    "                '✅ 注意力机制提供解释性',\n",
    "                '✅ 能捕获复杂的长期依赖',\n",
    "                '✅ 在复杂数据上表现优秀'\n",
    "            ],\n",
    "            \n",
    "            # 劣势\n",
    "            'disadvantages': [\n",
    "                '⚠️ 计算复杂度O(n²)',\n",
    "                '⚠️ 参数量大，容易过拟合',\n",
    "                '⚠️ 对长序列内存消耗大',\n",
    "                '⚠️ 需要更多调参'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'CNN-Attention': {\n",
    "            # 训练配置\n",
    "            'use_mixed_precision': True,\n",
    "            'gradient_clip_norm': 0.5,\n",
    "            'training_reason': '混合架构，推荐保守的梯度裁剪配置。',\n",
    "            \n",
    "            # 推荐条件\n",
    "            'recommend_when': [\n",
    "                ('need_local_and_global == True', '需要同时捕获局部和全局特征'),\n",
    "                ('computational_budget == \"medium\"', '中等计算资源'),\n",
    "                ('experimental == True', '实验性尝试'),\n",
    "            ],\n",
    "            \n",
    "            # 适用场景\n",
    "            'best_for': [\n",
    "                '局部和全局特征都重要的数据',\n",
    "                '实验和研究项目',\n",
    "                '需要平衡性能的场景'\n",
    "            ],\n",
    "            \n",
    "            # 优势\n",
    "            'advantages': [\n",
    "                '✅ 结合CNN和Attention优势',\n",
    "                '✅ 对局部模式敏感',\n",
    "                '✅ 有一定的全局建模能力'\n",
    "            ],\n",
    "            \n",
    "            # 劣势\n",
    "            'disadvantages': [\n",
    "                '⚠️ 架构复杂，调参困难',\n",
    "                '⚠️ 可能不如专门的模型',\n",
    "                '⚠️ 训练稳定性一般'\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Seed set to 42\n",
      "Model architecture:\n",
      "lstm.weight_ih_l0              | Shape: torch.Size([256, 21]) | Parameters: 5376 | require_grad: True\n",
      "Mean Value: -0.00018090897356159985 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.weight_hh_l0              | Shape: torch.Size([256, 64]) | Parameters: 16384 | require_grad: True\n",
      "Mean Value: -0.0004572543257381767 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.bias_ih_l0                | Shape: torch.Size([256]) | Parameters: 256 | require_grad: True\n",
      "Mean Value: 0.006879021879285574 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.bias_hh_l0                | Shape: torch.Size([256]) | Parameters: 256 | require_grad: True\n",
      "Mean Value: 0.006068169604986906 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.0.weight             | Shape: torch.Size([64]) | Parameters: 64 | require_grad: True\n",
      "Mean Value: 1.0 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.0.bias               | Shape: torch.Size([64]) | Parameters: 64 | require_grad: True\n",
      "Mean Value: 0.0 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.1.weight             | Shape: torch.Size([32, 64]) | Parameters: 2048 | require_grad: True\n",
      "Mean Value: -0.00021209358237683773 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.1.bias               | Shape: torch.Size([32]) | Parameters: 32 | require_grad: True\n",
      "Mean Value: -0.010356508195400238 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.4.weight             | Shape: torch.Size([1, 32]) | Parameters: 32 | require_grad: True\n",
      "Mean Value: 0.01477556861937046 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.4.bias               | Shape: torch.Size([1]) | Parameters: 1 | require_grad: True\n",
      "Mean Value: 0.16345524787902832 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "Model architecture:\n",
      "lstm.weight_ih_l0              | Shape: torch.Size([256, 21]) | Parameters: 5376 | require_grad: True\n",
      "Mean Value: -0.00018090897356159985 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.weight_hh_l0              | Shape: torch.Size([256, 64]) | Parameters: 16384 | require_grad: True\n",
      "Mean Value: -0.0004572543257381767 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.bias_ih_l0                | Shape: torch.Size([256]) | Parameters: 256 | require_grad: True\n",
      "Mean Value: 0.006879021879285574 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "lstm.bias_hh_l0                | Shape: torch.Size([256]) | Parameters: 256 | require_grad: True\n",
      "Mean Value: 0.006068169604986906 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.0.weight             | Shape: torch.Size([64]) | Parameters: 64 | require_grad: True\n",
      "Mean Value: 1.0 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.0.bias               | Shape: torch.Size([64]) | Parameters: 64 | require_grad: True\n",
      "Mean Value: 0.0 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.1.weight             | Shape: torch.Size([32, 64]) | Parameters: 2048 | require_grad: True\n",
      "Mean Value: -0.00021209358237683773 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.1.bias               | Shape: torch.Size([32]) | Parameters: 32 | require_grad: True\n",
      "Mean Value: -0.010356508195400238 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.4.weight             | Shape: torch.Size([1, 32]) | Parameters: 32 | require_grad: True\n",
      "Mean Value: 0.01477556861937046 | Grad: None\n",
      "--------------------------------------------------------------------------------\n",
      "fc_layers.4.bias               | Shape: torch.Size([1]) | Parameters: 1 | require_grad: True\n",
      "Mean Value: 0.16345524787902832 | Grad: None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from config import K, HIDDEN_DIM, LSTM_HIDDEN_DIM, DROPOUT_RATE, DEVICE\n",
    "from architecture import TwoLayerLSTM, TCNModel, DLinear, LightTransformer,CNNAttentionModel\n",
    "from seed import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "lstm = TwoLayerLSTM(k=K, hidden_dim=HIDDEN_DIM, lstm_hidden_dim=LSTM_HIDDEN_DIM, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "print(\"Model architecture:\")\n",
    "for name, param in lstm.named_parameters():\n",
    "    print(f\"{name:30s} | Shape: {param.data.shape} | Parameters: {param.numel()} | require_grad: {param.requires_grad}\")\n",
    "    print(f\"Mean Value: {param.data.mean()} | Grad: {param.grad}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "#长序列，TCN处理效率最高\n",
    "lstm = TCNModel(k=K, hidden_dim=HIDDEN_DIM, num_layers=4, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "\n",
    "# 选项2：DLinear (数据有规律模式时)\n",
    "set_seed(42)\n",
    "lstm = DLinear(k=K, lookback=LOOKBACK, individual=True).to(DEVICE)\n",
    "\n",
    "# 选项3：Transformer (中等长度序列)\n",
    "set_seed(42)\n",
    "lstm = LightTransformer(k=K, d_model=HIDDEN_DIM, nhead=4, num_layers=2, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "\n",
    "# 选项4:CNN-Attention\n",
    "lstm = CNNAttentionModel(\n",
    "        k=K, \n",
    "        hidden_dim=HIDDEN_DIM, \n",
    "        num_heads=4,            # 注意力头数\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    ).to(DEVICE)\n",
    "\"\"\"\n",
    "print(\"Model architecture:\")\n",
    "for name, param in lstm.named_parameters():\n",
    "    print(f\"{name:30s} | Shape: {param.data.shape} | Parameters: {param.numel()} | require_grad: {param.requires_grad}\")\n",
    "    print(f\"Mean Value: {param.data.mean()} | Grad: {param.grad}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_data_loaders\n",
    "# 创建 Subset 类\n",
    "from torch.utils.data import DataLoader\n",
    "from config import BATCH_SIZE\n",
    "# 假设 dataset_train, dataset_test 都是 optDataset 的实例\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "loader_test  = DataLoader(dataset_test,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "loader_train, loader_test = get_data_loaders(dataset_train, dataset_test)\n",
    "\n",
    "loss_log=[]\n",
    "loss_log_regret=[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from train import trainModel_with_log\n",
    "from config import NUM_EPOCHS, BATCH_SIZE, LR, LSTM_SAVE_DIR\n",
    "import os\n",
    "os.makedirs(LSTM_SAVE_DIR, exist_ok=True)   \n",
    "import pyepo\n",
    "import torch \n",
    "# 创建带时间戳的实验目录\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"spo_plus_experiment_{timestamp}\"\n",
    "log_dir = f\"./logs/{experiment_name}\"\n",
    "print(f\"实验名称: {experiment_name}\")\n",
    "print(f\"TensorBoard日志目录: {log_dir}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "spop = pyepo.func.SPOPlus(model)\n",
    "loss_log, loss_log_regret = trainModel_with_log(\n",
    "    lstm, \n",
    "    loss_func=spop, \n",
    "    method_name=\"spo+\",\n",
    "    loader_train=loader_train,\n",
    "    loader_test=loader_test,\n",
    "    market_neutral_model=model,\n",
    "    params_testing=params_testing,\n",
    "    loss_log=loss_log, \n",
    "    loss_log_regret=loss_log_regret,\n",
    "    num_epochs=NUM_EPOCHS,  # Increased for better convergence\n",
    "    lr=LR,\n",
    "    initial=True,# Adjusted learning rate\n",
    "    scaler=True,\n",
    "    tensorboard_log_dir=log_dir  # 使用我们创建的目录\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 启动TensorBoard查看结果\n",
    "# =============================================================================\n",
    "\n",
    "# 方法1: 使用魔法命令（推荐）\n",
    "%tensorboard --logdir $log_dir\n",
    "\n",
    "# 或者方法2: 使用notebook.start()\n",
    "# notebook.start(\"--logdir \" + log_dir)\n",
    "\n",
    "print(\"\\n训练完成！TensorBoard已启动，您可以在上方查看训练曲线。\")\n",
    "print(f\"如果TensorBoard没有显示，请运行: %tensorboard --logdir {log_dir}\")\n",
    "\n",
    "lstm_save_path = os.path.join(LSTM_SAVE_DIR, \"trial_7_assets.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': lstm.state_dict(),\n",
    "    'loss_log': loss_log,\n",
    "    'loss_log_regret': loss_log_regret\n",
    "}, lstm_save_path)\n",
    "\n",
    "print(f\"模型参数已保存到 {lstm_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Num of cores: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  76%|███████▌  | 9962/13101 [01:25<00:27, 115.14it/s, loss=0.0000, batch time=0.0063s] "
     ]
    }
   ],
   "source": [
    "from train import trainModel\n",
    "from config import NUM_EPOCHS, BATCH_SIZE, LR, LSTM_SAVE_DIR\n",
    "import os\n",
    "os.makedirs(LSTM_SAVE_DIR, exist_ok=True)   \n",
    "import pyepo\n",
    "import torch \n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "spop = pyepo.func.SPOPlus(model)\n",
    "loss_log, loss_log_regret = trainModel(\n",
    "    lstm, \n",
    "    loss_func=spop, \n",
    "    method_name=\"spo+\",\n",
    "    loader_train=loader_train,\n",
    "    loader_test=loader_test,\n",
    "    market_neutral_model=model,\n",
    "    params_testing=params_testing,\n",
    "    loss_log=loss_log, \n",
    "    loss_log_regret=loss_log_regret,\n",
    "    num_epochs=NUM_EPOCHS,  # Increased for better convergence\n",
    "    lr=LR,\n",
    "    initial=False,# Adjusted learning rate,\n",
    "    scaler= False,\n",
    ")\n",
    "\n",
    "lstm_save_path = os.path.join(LSTM_SAVE_DIR, \"trial_7_assets.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': lstm.state_dict(),\n",
    "    'loss_log': loss_log,\n",
    "    'loss_log_regret': loss_log_regret\n",
    "}, lstm_save_path)\n",
    "\n",
    "print(f\"模型参数已保存到 {lstm_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_save_path = os.path.join(LSTM_SAVE_DIR, \"trial_7_assets.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': lstm.state_dict(),\n",
    "    'loss_log': loss_log,\n",
    "    'loss_log_regret': loss_log_regret\n",
    "}, lstm_save_path)\n",
    "\n",
    "print(f\"模型参数已保存到 {lstm_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import trainModel\n",
    "from config import NUM_EPOCHS, BATCH_SIZE, LR, LSTM_SAVE_DIR\n",
    "import os\n",
    "os.makedirs(LSTM_SAVE_DIR, exist_ok=True)   \n",
    "import pyepo\n",
    "import torch \n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    print( f\"第{i+1}次训练\") \n",
    "\n",
    "    set_seed(42)\n",
    "    \n",
    "    spop = pyepo.func.SPOPlus(model)\n",
    "    loss_log, loss_log_regret = trainModel(\n",
    "        lstm, \n",
    "        loss_func=spop, \n",
    "        method_name=\"spo+\",\n",
    "        loader_train=loader_train,\n",
    "        loader_test=loader_test,\n",
    "        market_neutral_model=model,\n",
    "        params_testing=params_testing,\n",
    "        loss_log=loss_log, \n",
    "        loss_log_regret=loss_log_regret,\n",
    "        num_epochs=NUM_EPOCHS,  # Increased for better convergence\n",
    "        lr=LR,\n",
    "        initial=False,# Adjusted learning rate,\n",
    "        scaler= False\n",
    "    )\n",
    "    \n",
    "    lstm_save_path = os.path.join(LSTM_SAVE_DIR, \"trial_7_assets.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': lstm.state_dict(),\n",
    "        'loss_log': loss_log,\n",
    "        'loss_log_regret': loss_log_regret\n",
    "    }, lstm_save_path)\n",
    "    \n",
    "    print(f\"模型参数已保存到 {lstm_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# VISUALIZATION\n",
    "#############################################################################\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    \"\"\"Enhanced visualization with smoother curves and more information\"\"\"\n",
    "    # Create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n",
    "    \n",
    "    # Plot training loss with smoothing for readability\n",
    "    n_points = len(loss_log)\n",
    "    \n",
    "    # Apply smoothing for large datasets\n",
    "    if n_points > 100:\n",
    "        window_size = max(10, n_points // 50)\n",
    "        smoothed_loss = np.convolve(loss_log, np.ones(window_size)/window_size, mode='valid')\n",
    "        x_axis = np.arange(len(smoothed_loss))\n",
    "        ax1.plot(x_axis, smoothed_loss, color=\"c\", lw=2, label=f\"Smoothed (window={window_size})\")\n",
    "        # Also plot the raw data with transparency\n",
    "        ax1.plot(loss_log, color=\"c\", lw=0.5, alpha=0.3, label=\"Raw\")\n",
    "        ax1.legend()\n",
    "    else:\n",
    "        # For smaller datasets, just plot the raw data\n",
    "        ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    \n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iterations\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Draw plot for regret on test set\n",
    "    epochs = np.arange(len(loss_log_regret))\n",
    "    ax2.plot(epochs, [r*100 for r in loss_log_regret], \n",
    "             color=\"royalblue\", marker='o', ls=\"-\", alpha=0.8, lw=2)\n",
    "    \n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, max(50, max([r*100 for r in loss_log_regret])*1.1))  # Dynamic y-limit\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret (%)\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values to points\n",
    "    for i, r in enumerate(loss_log_regret):\n",
    "        ax2.annotate(f\"{r*100:.2f}%\", \n",
    "                     (i, r*100),\n",
    "                     textcoords=\"offset points\", \n",
    "                     xytext=(0,10), \n",
    "                     ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "    # print(\"Saved learning curves to 'learning_curves.png'\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing learning curves...\")\n",
    "visLearningCurve(loss_log, loss_log_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import trainModel\n",
    "from config import NUM_EPOCHS, BATCH_SIZE, LR, LSTM_SAVE_DIR\n",
    "import pyepo\n",
    "import torch \n",
    "import os\n",
    "\n",
    "lstm_save_path = os.path.join(LSTM_SAVE_DIR, \"trial_7_assets.pt\")\n",
    "\n",
    "lstm_eval = TwoLayerLSTM(k=K, hidden_dim=HIDDEN_DIM, lstm_hidden_dim=LSTM_HIDDEN_DIM, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lstm_eval = TCNModel(k=K, hidden_dim=HIDDEN_DIM, num_layers=4, dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "checkpoint = torch.load(lstm_save_path, map_location=DEVICE, weights_only=False)\n",
    "lstm_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Model architecture after training:\")\n",
    "for name, param in lstm_eval.named_parameters():\n",
    "    print(f\"{name:30s} | Shape: {param.data.shape} | Parameters: {param.numel()} | require_grad: {param.requires_grad}\")\n",
    "    print(f\"Mean Value: {param.data.mean()} | Grad: {param.grad}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# set lstm to evaluation mode\n",
    "lstm_eval.eval()\n",
    "\n",
    "pred_returns = [] # predicted returns\n",
    "with torch.no_grad():\n",
    "    for x, _, _, _ in loader_test:\n",
    "        # 1. 预测 returns\n",
    "        x = x.to(DEVICE)                      # (batch_size, N, lookback, k)\n",
    "        pred_return = lstm_eval(x)                   # (batch_size, N)\n",
    "        pred_return = pred_return.cpu().numpy()             # 转成ndarray\n",
    "        pred_returns.append(pred_return)\n",
    "\n",
    "pred_returns = np.vstack(pred_returns) # (T, N)\n",
    "print(f\"预测的收益率矩阵形状是: {pred_returns.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"标准化前的统计:\")\n",
    "print(f\"  全局均值: {np.mean(pred_returns):.6f}\")\n",
    "print(f\"  正数比例: {(pred_returns > 0).mean()*100:.1f}%\")\n",
    "\n",
    "# 对每个时间步进行横截面标准化\n",
    "mean_cross_section = np.mean(pred_returns, axis=1, keepdims=True)  # (T, 1)\n",
    "std_cross_section = np.std(pred_returns, axis=1, keepdims=True)    # (T, 1)\n",
    "pred_returns_standardized = (pred_returns - mean_cross_section) / (std_cross_section + 1e-8)\n",
    "\n",
    "print(\"标准化后的统计:\")\n",
    "print(f\"  全局均值: {np.mean(pred_returns_standardized):.6f}\")\n",
    "print(f\"  正数比例: {(pred_returns_standardized > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_test = np.load(\"test_data/features_crypto_data.npy\")\n",
    "print(f\"测试集feature的形状是: {feats_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 优化器\n",
    "from config import PRECOMPUTE_BATCH_SIZE, LOOKBACK, PADDING_METHOD\n",
    "from batch_runner import process_and_combine_shared\n",
    "\n",
    "\n",
    "#dataset_dict_test = process_and_combine_shared(feats_test, pred_returns, batch_size=PRECOMPUTE_BATCH_SIZE, **params)\n",
    "from test_regret import sequential_solutions\n",
    "\n",
    "positions=sequential_solutions(predmodel=lstm_eval,params_testing=params_testing, dataloader=loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集仓位\n",
    "#positions = dataset_dict_test[\"sols\"]\n",
    "positions=np.array(positions)\n",
    "print(f\"测试集仓位的形状是: {positions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl_series = []\n",
    "for i in range(len(positions)):\n",
    "    pnl = np.nansum(positions[i] * test_dataset_dict[\"costs\"][i])\n",
    "    pnl_series.append(pnl)\n",
    "print(f\"pnl series的长度是: {len(pnl_series)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cumulative Pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_cum_pnl_series(pnl_series, title='Cumulative PnL', figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    绘制累计 PnL 曲线。\n",
    "\n",
    "    参数\n",
    "    -----\n",
    "    pnl_series : pandas.Series\n",
    "        每期的 PnL 序列，索引为日期或时间点。\n",
    "    title : str, optional\n",
    "        图表标题，默认 'Cumulative PnL'。\n",
    "    figsize : tuple, optional\n",
    "        图表大小，默认为 (10, 6)。\n",
    "    \"\"\"\n",
    "    # 计算累计 PnL\n",
    "    cum_pnl = pd.Series(pnl_series).cumsum()\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(np.arange(len(cum_pnl)), cum_pnl.values, linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative PnL')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cum_pnl_series(pnl_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_returns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 智能检测你的数据格式\n",
    "def smart_pnl_plot(pred_returns):\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    if hasattr(pred_returns, 'cpu'):\n",
    "        data = pred_returns.cpu().numpy()\n",
    "    else:\n",
    "        data = np.array(pred_returns)\n",
    "    \n",
    "    print(f\"数据形状: {data.shape}\")\n",
    "    \n",
    "    if data.ndim == 2:\n",
    "        # 判断是 (时间, 资产) 还是 (资产, 时间)\n",
    "        if data.shape[0] > data.shape[1]:\n",
    "            # 假设是 (时间, 资产)\n",
    "            print(\"格式: (时间, 资产)\")\n",
    "            avg_returns = np.mean(data, axis=1)  # 每个时间步的平均收益\n",
    "        else:\n",
    "            # 假设是 (资产, 时间)\n",
    "            print(\"格式: (资产, 时间)\")\n",
    "            avg_returns = np.mean(data, axis=0)  # 每个时间步的平均收益\n",
    "    else:\n",
    "        print(f\"数据维度: {data.ndim}\")\n",
    "        return None\n",
    "    \n",
    "    # 计算累计PnL\n",
    "    pnl = np.cumsum(avg_returns)\n",
    "    \n",
    "    # 绘图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(pnl, linewidth=2)\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.title('Portfolio PnL')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Cumulative PnL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"最终PnL: {pnl[-1]:.6f}\")\n",
    "    return pnl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用你的数据\n",
    "smart_pnl_plot(pred_returns_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用你的数据\n",
    "smart_pnl_plot(test_dataset_dict[\"costs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
